{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHF_TOKEN\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.getenv(\u001b[33m'\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mLLAMA_INDEX_TOKEN\u001b[39m\u001b[33m'\u001b[39m] = os.getenv(\u001b[33m'\u001b[39m\u001b[33mLLAMA_INDEX_TOKEN\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m'\u001b[39m] = os.getenv(\u001b[33m'\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:684\u001b[39m, in \u001b[36m__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:758\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(value)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "os.environ['LLAMA_INDEX_TOKEN'] = os.getenv('LLAMA_INDEX_TOKEN')\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llx-u9CgBTDcgirzPcI6v5khnRFR01MdhAmPBn4P9gM1i1C73bLt\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "llama_cloud_api_key = os.environ['LLAMA_INDEX_TOKEN']\n",
    "print(llama_cloud_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 96a416ea-e201-466c-a7c1-2ca4345faac3\n"
     ]
    }
   ],
   "source": [
    "documents = LlamaParse(\n",
    "    api_key=llama_cloud_api_key,\n",
    "    result_type=\"markdown\",\n",
    "    content_guideline_instruction=\"This is a resume, gather relevant facts together and format them as bullet points with headers\",\n",
    ").load_data(\n",
    "    \"data/resume.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Aman Lonare\n",
      "\n",
      "Email: amanlonare95@gmail.com\n",
      "\n",
      "Phone: +81‑70‑9018‑5707\n",
      "\n",
      "Location: Tokyo, Japan\n",
      "\n",
      "# PROFESSIONAL EXPERIENCE\n",
      "\n",
      "# Hitachi R&D, Center for Technology Innovation\n",
      "\n",
      "Tokyo, JPN\n",
      "\n",
      "Research Software Engineering, Services Computing Research Dept.\n",
      "Jan 2021 – Present\n",
      "\n",
      "- Developed a tool in Python for assisting in the implementation of CQRS and Event Sourcing design patterns for business users\n",
      "- Designed and performed evaluation of Core Banking application using AxonIQ Framework with Prometheus and Grafana\n",
      "- Reduced the overall development time by 15% using Domain Driven Design (DDD) principles of software development process\n",
      "- Facilitated the implementation of event driven architecture in system and integrated Machine Learning solutions with the tool\n",
      "- Proposed a plan to integrate the developed tool in Hitachi’s indigenous framework for microservices application development\n",
      "\n",
      "Patent: “System and method to assist the modelling of CQRS and Event Sourcing based application”, In preparation\n",
      "\n",
      "# Indian Institute of Technology Bombay\n",
      "\n",
      "Mumbai, IND\n",
      "\n",
      "Project Research Associate, Technology & Development\n",
      "Sept 2020 – Nov 2020\n",
      "\n",
      "- Developed models for detecting onions infection using YOLOv3 and different sensors data with an overall accuracy of 95%\n",
      "- Early detection reduced the food spoilage by 20% and increased farmers revenue by 10% using Computer Vision techniques\n",
      "- Formulated a two year curriculum for a nationwide course on “Digital Technology (Robotics) for Smart Agriculture” by ICAR\n",
      "\n",
      "# Home Equity Private Limited, WhatsLoan\n",
      "\n",
      "Bangalore, IND\n",
      "\n",
      "Software Developer Intern\n",
      "May 2016 – July 2016\n",
      "\n",
      "- Developed functional prototypes for Android application of the organization which includes Login, and Registration screens\n",
      "- Enabled data visualization of the user characteristics by integrating the Google Analytics for optimizing the user acquisition\n",
      "\n",
      "# PROJECTS\n",
      "\n",
      "# Web-based Decision Support System (DSS) for Crop Monitoring\n",
      "\n",
      "Aug 2019 – July 2020\n",
      "\n",
      "- Developed a web portal for farmers, government, and sugarcane mills for real-time crop monitoring using remote sensing\n",
      "- Achieved an accuracy of 78% and F1-score of 0.8 in predicting crop yields with CNN using open source satellite images\n",
      "- Reported the increase in the MSP of sugarcane by 15% from improved supply chain and reduced spoilage by using the portal\n",
      "- Publication: Lonare, A., Maheshwari, B., Chinnasamy, P. (2020). Village Level Identification of Sugarcane Crop in India using Open Source Satellite Images. Acta Geophysica. Submitted for publication\n",
      "\n",
      "# SKILLS\n",
      "\n",
      "- Programming Languages: Python, Java, SQL, HTML5, CSS3\n",
      "- Data Science & Technologies: A/B testing, ETL, Data Science Pipeline, Keras, Tensorflow, PyCharm, Docker, Kubernetes, AWS, Git, Flask, Axon Framework, Spring Boot, Microservices, DevOps, OSS\n",
      "\n",
      "# LEADERSHIP & AWARDS\n",
      "\n",
      "- Publication: Lonare, A., Srivastava, A., Chinnasamy, P. (2019). Study of LULC Change in Academic Campus by Analyzing Rainfall‑Runoff Process for Sustainable Design. [Conference Paper].\n",
      "- Winner of the TECHNO-VISION 2018, a National Level Technical Paper Presentation organized under TEQIP‑III, India\n",
      "- Setup Rural Data Research & Analysis Lab (RuDRA) as a Teaching Assistant (TA) in IIT Bombay with Rs. 2 million budgets\n",
      "\n",
      "# EDUCATION\n",
      "\n",
      "# Indian Institute of Technology Bombay\n",
      "\n",
      "Post Graduation\n",
      "Mumbai, IND\n",
      "\n",
      "Specialization: Technology and Development\n",
      "July 2018 - July 2020\n",
      "\n",
      "- Cumulative CGPA: 9.3/10\n",
      "- Machine Learning in Remote Sensing | Advanced Statistics | Satellite Image Processing | Project Management | Probability\n",
      "\n",
      "# Indian Institute of Technology Kanpur\n",
      "\n",
      "Graduation\n",
      "Kanpur, IND\n",
      "\n",
      "Specialization: Mechanical Engineering\n",
      "Aug 2013 - July 2017\n",
      "\n",
      "- Major Project: Fabrication of Non Destructive Testing Tool for Machinery Inspection\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=Settings.embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Groq(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x310575110>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x107a94ea0>, completion_to_prompt=<function default_completion_to_prompt at 0x107f172e0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='llama-3.2-3b-preview', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='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', api_base='https://api.groq.com/openai/v1', api_version='', strict=False, reasoning_effort=None, modalities=None, audio_config=None, context_window=3900, is_chat_model=True, is_function_calling_model=True, tokenizer=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"What is this person name and what was their most recent job\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"./storage\"\n",
    "index.storage_context.persist(\n",
    "    persist_dir=storage_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "if os.path.exists(storage_dir):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "    restored_index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    print(\"Index not found on disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person's name is Aman Lonare. Their most recent job is Research Software Engineering at Hitachi R&D, Center for Technology Innovation, where they have been working since January 2021.\n"
     ]
    }
   ],
   "source": [
    "response = restored_index.as_query_engine(llm=llm, similarity_top_k=5,).query(\n",
    "    \"What is this person name and what was their most recent job\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below function will be provided to the llm which will assess the docstring, name and metadata of the function and decides if it useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_resume(q: str):\n",
    "    \"\"\"Answer questions about a specific resume.\"\"\"\n",
    "    # we are using query engine we created from above\n",
    "    response = query_engine.query(f\"This is a question about the specific resume: {q}\")\n",
    "    return response.response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will turn the method into a tool using FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resume_tool = FunctionTool.from_defaults(\n",
    "    fn=query_resume,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will create a function calling agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tools=[resume_tool], # just one in our case\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e3944d25-a673-448c-9c7b-d3a7dd6809e9. Step input: How many years of experience does this person have? Give me exact number of years as output\n",
      "Added user message to memory: How many years of experience does this person have? Give me exact number of years as output\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"years of experience\"}\n",
      "=== Function Output ===\n",
      "The individual has around 7 years of experience in the field of software engineering and technology development.\n",
      "> Running step 7d527efa-7988-4a0f-a12d-85b8789a4965. Step input: None\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"number of years of experience\"}\n",
      "=== Function Output ===\n",
      "Based on the provided context information, Aman Lonare has approximately 3 years of experience in the field of Research Software Engineering at Hitachi R&D, Center for Technology Innovation.\n",
      "> Running step 8ca46822-d39b-48ea-bea2-b7ad1466ca5e. Step input: None\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"number of years of experience in Research Software Engineering\"}\n",
      "=== Function Output ===\n",
      "The individual has around 2 years of experience in Research Software Engineering, starting from January 2021.\n",
      "> Running step 4f5bd1e0-779f-44ac-8976-fd5f00f2d14a. Step input: None\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"experience in Research Software Engineering\"}\n",
      "=== Function Output ===\n",
      "The individual has experience in developing tools for business users, specifically a tool in Python for assisting in the implementation of CQRS and Event Sourcing design patterns. They have also designed and performed evaluations of Core Banking applications using the AxonIQ Framework with Prometheus and Grafana. Additionally, they have reduced development time by 15% using Domain Driven Design principles and facilitated the implementation of event-driven architecture in systems.\n",
      "> Running step 81375493-8c36-47b8-8026-6aeca049f1ad. Step input: None\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"experience in developing tools for business users\"}\n",
      "=== Function Output ===\n",
      "The individual has experience in developing tools for business users, specifically a tool in Python for assisting in the implementation of CQRS and Event Sourcing design patterns. This experience was gained while working as a Research Software Engineering at Hitachi R&D, Center for Technology Innovation.\n",
      "> Running step 8e9db39c-93d9-4afe-9c4c-779f58615bd2. Step input: None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How many years of experience does this person have? Give me exact number of years as output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEvent(Event):\n",
    "    \"\"\"Custom event to query the resume.\"\"\"\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    storage_dir = \"./storage\"\n",
    "    llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "    query_engine = VectorStoreIndex\n",
    "\n",
    "    # first step will be setup\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "        \n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"Resume file not provided\")\n",
    "        \n",
    "        # define an llm to work with\n",
    "        self.llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "\n",
    "        # ingest data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse the data if storage index is not created\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather relevant facts together and format them as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=Settings.embed_model,\n",
    "            )\n",
    "            # persist the index\n",
    "            index.storage_context.persist(\n",
    "                persist_dir=self.storage_dir,\n",
    "            )\n",
    "        \n",
    "        # either way, create query engine\n",
    "        self.query_engine = index.as_query_engine(\n",
    "            llm=self.llm,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "\n",
    "        # fire off the query event defined\n",
    "        return QueryEvent(query=ev.query)\n",
    "    \n",
    "    # second step will be to ask a question and return a result immediately\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        # query the resume\n",
    "        response = self.query_engine.query(f\"This is the question about the specific resume: {ev.query}\")\n",
    "        # return the result\n",
    "        return StopEvent(result=response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitachi R&D, Center for Technology Innovation in Tokyo, Japan.\n"
     ]
    }
   ],
   "source": [
    "workflow = RAGWorkflow(timeout=60, verbose=False)\n",
    "result = await workflow.run(\n",
    "    resume_file=\"data/resume.pdf\",\n",
    "    query=\"Where is the first place the person worked?\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=llama_cloud_api_key,\n",
    "    content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "    formatting_instruction=\"Return a bulleted list of the fields ONLY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id eb53bb03-7362-4c53-817d-53e55e28eba2\n"
     ]
    }
   ],
   "source": [
    "result = parser.load_data(\n",
    "    file_path=\"data/application_form.pdf\",\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name:\n",
      "Last Name:\n",
      "Email:\n",
      "Phone:\n",
      "LinkedIn:\n",
      "Project Portfolio:\n",
      "Degree:\n",
      "Done Post Graduation?:\n",
      "Post Graduation Date:\n",
      "Graduation Date:\n",
      "Current Job Title:\n",
      "Current Employer:\n",
      "Technical Skills:\n",
      "Describe why the candidate is a good fit for this position:\n",
      "Do you have 5 years of experience in Python?:\n"
     ]
    }
   ],
   "source": [
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "raw_json = llm.complete(\n",
    "    f\"\"\"\n",
    "    This is a parsed from a job application form.\n",
    "    Convert it into a JSON object containing only the list of fields\n",
    "    to be filled in, in the form {{\"fields\": [...]}}.\n",
    "    <form>{result.text}</form>\n",
    "    Return JSON only, no markdown please. \n",
    "    Clean the text by removing '\\n' or new line if there are any.\n",
    "    Do not add the type and just return the field name as a list\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='{\"fields\": [\"First Name\", \"Last Name\", \"Email\", \"Phone\", \"LinkedIn\", \"Project Portfolio\", \"Degree\", \"Done Post Graduation?\", \"Post Graduation Date\", \"Graduation Date\", \"Current Job Title\", \"Current Employer\", \"Technical Skills\", \"Describe why the candidate is a good fit for this position\", \"Do you have 5 years of experience in Python?\"]}', additional_kwargs={'prompt_tokens': 190, 'completion_tokens': 84, 'total_tokens': 274}, raw=ChatCompletion(id='chatcmpl-ea8a58c1-ed0d-49fc-8fea-7c0c217063d3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"fields\": [\"First Name\", \"Last Name\", \"Email\", \"Phone\", \"LinkedIn\", \"Project Portfolio\", \"Degree\", \"Done Post Graduation?\", \"Post Graduation Date\", \"Graduation Date\", \"Current Job Title\", \"Current Employer\", \"Technical Skills\", \"Describe why the candidate is a good fit for this position\", \"Do you have 5 years of experience in Python?\"]}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1743583490, model='llama-3.2-3b-preview', object='chat.completion', service_tier=None, system_fingerprint='fp_a926bfdce1', usage=CompletionUsage(completion_tokens=84, prompt_tokens=190, total_tokens=274, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.231505342, prompt_time=0.031232414, completion_time=0.054191738, total_time=0.085424152), usage_breakdown={'models': None}, x_groq={'id': 'req_01jqttsbvmfhrr900ac52z2kxg'}), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name\n",
      "Last Name\n",
      "Email\n",
      "Phone\n",
      "LinkedIn\n",
      "Project Portfolio\n",
      "Degree\n",
      "Done Post Graduation?\n",
      "Post Graduation Date\n",
      "Graduation Date\n",
      "Current Job Title\n",
      "Current Employer\n",
      "Technical Skills\n",
      "Describe why the candidate is a good fit for this position\n",
      "Do you have 5 years of experience in Python?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "fields = json.loads(raw_json.text)['fields']\n",
    "\n",
    "for field in fields:\n",
    "    print(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    \"\"\"Custom event to parse the form.\"\"\"\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    \"\"\"Custom event to query the resume.\"\"\"\n",
    "    query: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    \"\"\"Custom event to return the response.\"\"\"\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormParserRAGWorkflow(Workflow):\n",
    "    storage_dir = \"./storage\"\n",
    "    llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "    query_engine = VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "        \n",
    "        # define an llm to work with\n",
    "        self.llm = Groq(model=\"llama-3.2-3b-preview\", api_key=groq_api_key)\n",
    "\n",
    "        # ingest data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse the data if storage index is not created\n",
    "            parser = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather relevant facts together and format them as bullet points with headers\",\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=Settings.embed_model,\n",
    "            )\n",
    "            # persist the index\n",
    "            index.storage_context.persist(\n",
    "                persist_dir=self.storage_dir,\n",
    "            )\n",
    "        \n",
    "        # either way, create query engine\n",
    "        self.query_engine = index.as_query_engine(\n",
    "            llm=self.llm,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "\n",
    "        # fire off the query event defined\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "    \n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> QueryEvent:\n",
    "        \n",
    "        parser = LlamaParse(\n",
    "        api_key=llama_cloud_api_key,\n",
    "        content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "        formatting_instruction=\"Return a bulleted list of the fields ONLY\"\n",
    "        )\n",
    "\n",
    "        result = parser.load_data(\n",
    "            file_path=\"data/application_form.pdf\",\n",
    "        )[0]\n",
    "\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"\"\"\n",
    "            This is a parsed from a job application form.\n",
    "            Convert it into a JSON object containing only the list of fields\n",
    "            to be filled in, in the form {{\"fields\": [...]}}.\n",
    "            <form>{result.text}</form>\n",
    "            Return JSON only, no markdown please. \n",
    "            Clean the text by removing '\\n' or new line if there are any.\n",
    "            Do not add the type and just return the field name as a list\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        fields = json.loads(raw_json.text)['fields']\n",
    "\n",
    "        for field in fields:\n",
    "            ctx.send_event(\n",
    "                QueryEvent(\n",
    "                    field=field,\n",
    "                    query=f\"How would you answer the question about the candidate? Give me a succinct answer for each field if there is no description like first name etc.\",\n",
    "                    )\n",
    "            )\n",
    "    \n",
    "        await ctx.set(\"total fields\", len(fields))\n",
    "        return\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        # query the resume\n",
    "        response = self.query_engine.query(f\"This is the question about the specific resume: {ev.query}\")\n",
    "        # return the result\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "    \n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        total_fields = await ctx.get(\"total fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "\n",
    "        if responses is None:\n",
    "            return None\n",
    "        \n",
    "        responseList = \"\\n\".join(\"Field: \"+ r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in a job application form and the responses to questions about those fields from \n",
    "            a candidate's resume. Combine the two into a list of field and succinct, factual answers to fill in those fields. Do not\n",
    "            output 'Field' and 'Response' but fill in the values of them in the format Field: Response.\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        return StopEvent(result=result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id c18b91db-cef8-4720-a8da-f48320ff24ea\n",
      "Here is the combined list of fields and succinct, factual answers:\n",
      "\n",
      "1. Email: amanlonare95@gmail.com\n",
      "2. Phone: +81-70-9018-5707\n",
      "3. Location: Tokyo, Japan\n",
      "4. First Name: Not provided\n",
      "5. Last Name: Not provided\n",
      "6. Professional Experience:\n",
      "   - Research Software Engineering: Developed a tool for CQRS and Event Sourcing design patterns, designed evaluation of Core Banking application, reduced development time by 15%, facilitated event-driven architecture, and proposed tool integration.\n",
      "   - Project Research Associate: Developed models for detecting onion infection, formulated a nationwide course on Digital Technology for Smart Agriculture, and achieved high accuracy in detecting onions.\n",
      "   - Software Developer Intern: Developed functional prototypes for Android application, enabled data visualization using Google Analytics.\n",
      "7. Projects:\n",
      "   - Web-based Decision Support System: Developed a web portal for crop monitoring, achieved high accuracy in predicting crop yields, and reported increased MSP and reduced spoilage.\n",
      "   - Web-based Decision Support System: Developed a web portal for crop monitoring, achieved 78% accuracy in predicting crop yields, and reported increased MSP and reduced spoilage.\n",
      "8. Skills:\n",
      "   - Programming Languages: Python, Java, SQL, HTML5, CSS3\n",
      "   - Data Science & Technologies: A/B testing, ETL, Data Science Pipeline, Keras, Tensorflow, PyCharm, Docker, Kubernetes, AWS, Git, Flask, Axon Framework, Spring Boot, Microservices, DevOps, OSS\n",
      "9. Leadership & Awards:\n",
      "   - Publication: Study of LULC Change in Academic Campus\n",
      "   - Winner of TECHNO-VISION 2018\n",
      "   - Setup Rural Data Research & Analysis Lab\n",
      "10. Education:\n",
      "    - Post Graduation: Technology and Development, IIT Bombay\n",
      "    - Graduation: Mechanical Engineering, IIT Kanpur\n",
      "11. Field: LinkedIn\n",
      "12. Field: Project Portfolio\n",
      "13. Field: Degree\n",
      "14. Field: Done Post Graduation?\n",
      "15. Field: Post Graduation Date\n",
      "16. Field: Graduation Date\n",
      "17. Field: Current Job Title\n",
      "18. Field: Current Employer\n",
      "19. Field: Technical Skills\n",
      "20. Field: Describe why the candidate is a good fit for this position\n"
     ]
    }
   ],
   "source": [
    "workflow_form = FormParserRAGWorkflow(timeout=60, verbose=False)\n",
    "result = await workflow_form.run(\n",
    "    resume_file=\"data/resume.pdf\",\n",
    "    application_form=\"data/application_form.pdf\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
